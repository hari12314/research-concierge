{
    "test_session": {
        "pdf_text": "Providedproperattributionisprovided,Googleherebygrantspermissionto\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\nscholarlyworks.\nAttention Is All You Need\nAshishVaswani\u2217 NoamShazeer\u2217 NikiParmar\u2217 JakobUszkoreit\u2217\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones\u2217 AidanN.Gomez\u2217 \u2020 \u0141ukaszKaiser\u2217\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n\u2217Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n\u2020WorkperformedwhileatGoogleBrain.\n\u2021WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n3202\nguA\n2\n]LC.sc[\n7v26730.6071:viXra\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t\u22121\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\n2\nFigure1: TheTransformer-modelarchitecture.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlayers,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\n3\nScaledDot-ProductAttention Multi-HeadAttention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattentionlayersrunninginparallel.\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3.2.1 ScaledDot-ProductAttention\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\nqueriesandkeysofdimensiond\nk\n,a\u221andvaluesofdimensiond\nv\n. Wecomputethedotproductsofthe\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\nk\nvalues.\nInpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\nintoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\nthematrixofoutputsas:\nQKT\nAttention(Q,K,V)=softmax( \u221a )V (1)\nd\nk\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\nplicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\nof \u221a1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\ndk\nasinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\nmatrixmultiplicationcode.\nWhileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\nk\ndotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof\nk\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\nk\nextremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby \u221a1 .\ndk\n3.2.2 Multi-HeadAttention\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\nmodel\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\nk k v\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\nv\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\nvariableswithmean0andvariance1.Thentheirdotproduct,q\u00b7k=\n(cid:80)dk\nq k ,hasmean0andvarianced .\ni=1 i i k\n4\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepictedinFigure2.\nMulti-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\nsubspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.\nMultiHead(Q,K,V)=Concat(head ,...,head )WO\n1 h\nwherehead =Attention(QWQ,KWK,VWV)\ni i i i\nWheretheprojectionsareparametermatricesWQ \u2208Rdmodel\u00d7dk,WK \u2208Rdmodel\u00d7dk,WV \u2208Rdmodel\u00d7dv\ni i i\nandWO \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\nk v model\nissimilartothatofsingle-headattentionwithfulldimensionality.\n3.2.3 ApplicationsofAttentioninourModel\nTheTransformerusesmulti-headattentioninthreedifferentways:\n\u2022 In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\nandthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\npositioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38,2,9].\n\u2022 Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\nencoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\nencoder.\n\u2022 Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\nallpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward\ninformationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\ninsideofscaleddot-productattentionbymaskingout(settingto\u2212\u221e)allvaluesintheinput\nofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.\n3.3 Position-wiseFeed-ForwardNetworks\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\nconsistsoftwolineartransformationswithaReLUactivationinbetween.\nFFN(x)=max(0,xW +b )W +b (2)\n1 1 2 2\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is d = 512, and the inner-layer has dimensionality\nmodel\nd =2048.\nff\n3.4 EmbeddingsandSoftmax\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\ntokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-\nmodel\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-\u221asoftmax\nlineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d .\nmodel\n5\nTable1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\nfordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\nLayerType ComplexityperLayer Sequential MaximumPathLength\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(log (n))\nk\nSelf-Attention(restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 PositionalEncoding\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\norderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\ntokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\nbottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\nmodel\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\nlearnedandfixed[9].\nInthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\nPE =sin(pos/100002i/dmodel)\n(pos,2i)\nPE =cos(pos/100002i/dmodel)\n(pos,2i+1)\nwhereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\ncorrespondstoasinusoid. Thewavelengthsformageometricprogressionfrom2\u03c0to10000\u00b72\u03c0. We\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\nrelativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\npos+k\nPE .\npos\nWealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo\nversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\nduringtraining.\n4 WhySelf-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\n(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z \u2208 Rd, such as a hidden\n1 n 1 n i i\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\nconsiderthreedesiderata.\nOneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\ndependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\nandoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\ndifferentlayertypes.\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\n6\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\n[38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\ntheinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\nAsingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\norO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths\nk\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, toO(k\u00b7n\u00b7d+n\u00b7d2). Evenwithk = n, however, thecomplexityofaseparable\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\ntheapproachwetakeinourmodel.\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\nandsemanticstructureofthesentences.\n5 Training\nThissectiondescribesthetrainingregimeforourmodels.\n5.1 TrainingDataandBatching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\nvocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\ntargettokens.\n5.2 HardwareandSchedule\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\n(3.5days).\n5.3 Optimizer\nWeusedtheAdamoptimizer[20]with\u03b2 =0.9,\u03b2 =0.98and\u03f5=10\u22129. Wevariedthelearning\n1 2\nrateoverthecourseoftraining,accordingtotheformula:\nlrate=d\u22120.5 \u00b7min(step_num\u22120.5,step_num\u00b7warmup_steps\u22121.5) (3)\nmodel\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\nwarmup_steps=4000.\n5.4 Regularization\nWeemploythreetypesofregularizationduringtraining:\n7\nTable2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\nBLEU TrainingCost(FLOPs)\nModel\nEN-DE EN-FR EN-DE EN-FR\nByteNet[18] 23.75\nDeep-Att+PosUnk[39] 39.2 1.0\u00b71020\nGNMT+RL[38] 24.6 39.92 2.3\u00b71019 1.4\u00b71020\nConvS2S[9] 25.16 40.46 9.6\u00b71018 1.5\u00b71020\nMoE[32] 26.03 40.56 2.0\u00b71019 1.2\u00b71020\nDeep-Att+PosUnkEnsemble[39] 40.4 8.0\u00b71020\nGNMT+RLEnsemble[38] 26.30 41.16 1.8\u00b71020 1.1\u00b71021\nConvS2SEnsemble[9] 26.36 41.29 7.7\u00b71019 1.2\u00b71021\nTransformer(basemodel) 27.3 38.1 3.3\u00b71018\nTransformer(big) 28.4 41.8 2.3\u00b71019\nResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\nP =0.1.\ndrop\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue\u03f5 = 0.1[36]. This\nls\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\n6 Results\n6.1 MachineTranslation\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\nlistedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\nthecompetitivemodels.\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\ndropoutrateP =0.1,insteadof0.3.\ndrop\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\nusedbeamsearchwithabeamsizeof4andlengthpenalty\u03b1 = 0.6[38]. Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5.\n6.2 ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n8\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities.\ntrain PPL BLEU params\nN d d h d d P \u03f5\nmodel ff k v drop ls steps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n(A)\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n16 5.16 25.1 58\n(B)\n32 5.01 25.4 60\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n(C) 256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging. WepresenttheseresultsinTable3.\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\nk\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\nresultstothebasemodel.\n6.3 EnglishConstituencyParsing\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\nWetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\nmodel\nPennTreebank[25],about40Ktrainingsentences. Wealsotraineditinasemi-supervisedsetting,\nusingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences\n[37]. Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\nforthesemi-supervisedsetting.\nWeperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual\n(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\nofWSJ)\nParser Training WSJ23F1\nVinyals&Kaiserelal. (2014)[37] WSJonly,discriminative 88.3\nPetrovetal. (2006)[29] WSJonly,discriminative 90.4\nZhuetal. (2013)[40] WSJonly,discriminative 90.4\nDyeretal. (2016)[8] WSJonly,discriminative 91.7\nTransformer(4layers) WSJonly,discriminative 91.3\nZhuetal. (2013)[40] semi-supervised 91.3\nHuang&Harper(2009)[14] semi-supervised 91.3\nMcCloskyetal. (2006)[26] semi-supervised 92.1\nVinyals&Kaiserelal. (2014)[37] semi-supervised 92.1\nTransformer(4layers) semi-supervised 92.7\nLuongetal. (2015)[23] multi-task 93.0\nDyeretal. (2016)[8] generative 93.3\nincreasedthemaximumoutputlengthtoinputlength+300. Weusedabeamsizeof21and\u03b1=0.3\nforbothWSJonlyandthesemi-supervisedsetting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\nRecurrentNeuralNetworkGrammar[8].\nIncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-\nParser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.\n7 Conclusion\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\nattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\nmulti-headedself-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\nmodeloutperformsevenallpreviouslyreportedensembles.\nWeareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\nsuchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\ncomments,correctionsandinspiration.\nReferences\n[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\narXiv:1607.06450,2016.\n[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014.\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\nmachinetranslationarchitectures. CoRR,abs/1703.03906,2017.\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\nreading. arXivpreprintarXiv:1601.06733,2016.\n10\n[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\nmachinetranslation. CoRR,abs/1406.1078,2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprintarXiv:1610.02357,2016.\n[7] JunyoungChung,\u00c7aglarG\u00fcl\u00e7ehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetworkgrammars. InProc.ofNAACL,2016.\n[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850,2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition,pages770\u2013778,2016.\n[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJ\u00fcrgenSchmidhuber. Gradientflowin\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780,1997.\n[14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations\nacrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages832\u2013841.ACL,August2009.\n[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\n[16] \u0141ukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural\nInformationProcessingSystems,(NIPS),2016.\n[17] \u0141ukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\nonLearningRepresentations(ICLR),2016.\n[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\n2017.\n[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\nInInternationalConferenceonLearningRepresentations,2017.\n[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\n[21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\narXiv:1703.10722,2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130,2017.\n[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\nsequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\n[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\n11\n[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\ncorpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313\u2013330,1993.\n[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\npages152\u2013159.ACL,June2006.\n[27] AnkurParikh,OscarT\u00e4ckstr\u00f6m,DipanjanDas,andJakobUszkoreit. Adecomposableattention\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\n[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\nsummarization. arXivpreprintarXiv:1705.04304,2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433\u2013440.ACL,July\n2006.\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\npreprintarXiv:1608.05859,2016.\n[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.\n[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\nlayer. arXivpreprintarXiv:1701.06538,2017.\n[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\nLearningResearch,15(1):1929\u20131958,2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440\u20132448.CurranAssociates,\nInc.,2015.\n[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104\u20133112,2014.\n[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\n[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\nAdvancesinNeuralInformationProcessingSystems,2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google\u2019sneuralmachine\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\narXiv:1609.08144,2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n1: LongPapers),pages434\u2013443.ACL,August2013.\n12\nInput-Input Layer5\nAttentionVisualizations\ntI\ntI\nsi\nsi\nni\nni\nsiht\nsiht\ntirips\ntirips\ntaht\ntaht\na\na\nytirojam\nytirojam\nfo\nfo\nnaciremA\nnaciremA\nstnemnrevog\nstnemnrevog\nevah\nevah\ndessap\ndessap\nwen\nwen\nswal\nswal\necnis\necnis\n9002\n9002\ngnikam\ngnikam\neht\neht\nnoitartsiger\nnoitartsiger\nro\nro\ngnitov\ngnitov\nssecorp\nssecorp\nerom\nerom\ntluciffid\ntluciffid\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\ntheverb\u2018making\u2019,completingthephrase\u2018making...moredifficult\u2019. Attentionshereshownonlyfor\ntheword\u2018making\u2019. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\n13\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\nFullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword\u2018its\u2019forattentionheads5\nand6. Notethattheattentionsareverysharpforthisword.\n14\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\nsentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\natlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\n15\n"
    }
}